{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf40894",
   "metadata": {},
   "source": [
    "## Init For Agents Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb2b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added to sys.path: /home/snt/projects_lujun/LabAgentSkill/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/projects_lujun/LabAgentSkill/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - XBRL-tag-classification: Classify financial text into specific XBRL tags by analyzing semantic cues, context, and category boundaries.\n",
      "  - algorithmic-art: Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.\n",
      "  - brand-guidelines: Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.\n",
      "  - canvas-design: Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.\n",
      "  - doc-coauthoring: Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks.\n",
      "  - insurance-mail-triage: Triage long email threads for an insurance context, identify the most recent actionable message, extract key context, infer intent (e.g., missing documents, follow-up/reminder, payment, status), and decide whether to reply or take further action.\n",
      "  - movie-sentiment-analysis: Analyze text sentiment through concise reasoning steps using contextual understanding and keyword cues.\n"
     ]
    }
   ],
   "source": [
    "# Setup: Load environment variables and dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / \"src\"\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"✓ Added to sys.path: {src_path}\")\n",
    "\n",
    "from LabAgentSkill import skills_utils\n",
    "from LabAgentSkill.SkillAwareAgent import SkillAwareAgent\n",
    "\n",
    "root_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "env_path = root_dir / \".env\"\n",
    "env = {}\n",
    "\n",
    "if env_path.exists():\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        env[key.strip()] = value.strip()\n",
    "\n",
    "# Set API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = env.get(\"OPENAI_API_KEY\", os.environ.get(\"OPENAI_API_KEY\", \"\"))\n",
    "env = Environment(loader=FileSystemLoader('prompts/'))  \n",
    "skills_folder = Path(\"/home/snt/projects_lujun/LabAgentSkill/skillsHub/skills_insurBench\")\n",
    "all_skills = skills_utils.read_all_skills_metadata(skills_folder)\n",
    "for skill in all_skills:\n",
    "    print(f\"  - {skill['name']}: {skill['description']}\")\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "base_url = None\n",
    "\n",
    "\n",
    "# model_name = \"google/gemma-3-270m-it\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# base_url = \"http://127.0.0.1:8001/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5012e",
   "metadata": {},
   "source": [
    "## Load Data - Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82ea7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"/home/snt/projects_lujun/LabAgentSkill/assets/datasets/insureBench.jsonl\"\n",
    "loaded_df = pd.read_json(dataset_name, lines=True)\n",
    "loaded_df = loaded_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1529",
   "metadata": {},
   "source": [
    "## Select skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb474326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o-mini\n",
      "✓ SkillAwareAgent initialized\n",
      "  Model: gpt-4o-mini\n",
      "  Chat History: ENABLED ✓\n",
      "  Trim Messages: ENABLED ✓\n",
      "✓ SkillAwareAgent initialized\n",
      "  Model: gpt-4o-mini\n",
      "  Chat History: ENABLED ✓\n",
      "  Trim Messages: ENABLED ✓\n",
      "✓ SkillAwareAgent initialized\n",
      "  Model: gpt-4o-mini\n",
      "  Chat History: ENABLED ✓\n",
      "  Trim Messages: DISABLED ✗\n",
      "Results will be saved to: /home/snt/projects_lujun/LabAgentSkill/assets/results/insurBench_standard_gpt-4o-mini_20260212_210119.jsonl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loaded_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResume from row: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_row\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Process each sample\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm( \u001b[43mloaded_df\u001b[49m.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(loaded_df), desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing samples\u001b[39m\u001b[33m\"\u001b[39m,):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx < count_row:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'loaded_df' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from LabAgentSkill.evaluate import get_insurBench_predicted_label_v2, get_insurBench_predicted_label, get_predicted_label, get_prediction_XBRL_TAGS\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "# Initialize agents\n",
    "agent_skill_aware = SkillAwareAgent(use_chat_history=True, use_trim_messages=True, model=model_name, base_url=base_url)\n",
    "agent_skill_exec_agent = SkillAwareAgent(use_chat_history=True, use_trim_messages=True, model=model_name, base_url=base_url)\n",
    "agent_simple = SkillAwareAgent(use_chat_history=True, use_trim_messages=False, model=model_name, base_url=base_url)\n",
    "\n",
    "p_exec_insurBench_temp = env.get_template('p_exec_insurBench.jinja')\n",
    "p_skill_select_temp = env.get_template('p_skill_select.jinja')\n",
    "p_skill_discov_temp = env.get_template('p_skill_discov.jinja')\n",
    "p_default_system_temp = env.get_template('p_default_system.jinja')\n",
    "p_skill_exec_temp = env.get_template('p_skill_exec.jinja')\n",
    "\n",
    "# JSONL output path\n",
    "output_dir = \"/home/snt/projects_lujun/LabAgentSkill/assets/results/\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "jsonl_path = output_dir+f\"insurBench_standard_{model_name.split('/')[-1]}_{timestamp}.jsonl\"\n",
    "print(f\"Results will be saved to: {jsonl_path}\")\n",
    "\n",
    "skill_count = 0\n",
    "count_row = 0\n",
    "if os.path.exists(jsonl_path):\n",
    "    df_exist = pd.read_json(jsonl_path, lines=True)\n",
    "    count_row = len(df_exist)\n",
    "    print(f\"Resume from row: {count_row}\")\n",
    "\n",
    "\n",
    "# Process each sample\n",
    "for idx, row in tqdm( loaded_df.iterrows(), total=len(loaded_df), desc=\"Processing samples\",):\n",
    "\n",
    "    if idx < count_row:\n",
    "        continue\n",
    "    \n",
    "    sample_start_time, email_history, cls_label = time.time(), row.content, row.cls_label\n",
    "    true_label = {\"Non\": \"NO\", \"Oui\": \"YES\"}.get(cls_label, cls_label)\n",
    "\n",
    "    # Step 1: Skill Selection\n",
    "    # print(f\"Start Skill Selection Phase for Sample {idx + 1}/{len(loaded_df)}\")\n",
    "    skill_context = \"\\n\".join([\n",
    "        f\"- **{skill['name']}**: {skill['description']}\"\n",
    "        for skill in all_skills\n",
    "    ])\n",
    "\n",
    "    p_skill_select = p_skill_select_temp.render(SKILL_CONTEXT=skill_context)\n",
    "    p_exec_insurBench = p_exec_insurBench_temp.render(EMAIL_HISTORY = email_history)\n",
    "    skill_select_resp = agent_skill_aware.chat(user_input=p_exec_insurBench, custom_system_prompt=p_skill_select)\n",
    "    selected_skills = skills_utils.parse_skills_from_json_response(json_response=skill_select_resp, skills_hub_dir=skills_folder)\n",
    "\n",
    "    # Track whether \"movie-sentiment-analysis\" was selected in Step 1 \n",
    "    selected_skill_names_step1 = [s[\"name\"] for s in selected_skills]\n",
    "    hit_target_skill = \"insurance-mail-triage\".lower() in skill_select_resp.lower() ## This is hard Coded\n",
    "\n",
    "    skill_execution_context = \"\"\n",
    "    for skill_meta in selected_skills:\n",
    "        skill_execution_context += (\n",
    "            f\"SKill {skill_count + 1}: \\n\"\n",
    "            f\"{skill_meta['description']}\\n\"\n",
    "            f\"{'\\n'.join(skill_meta['body'].split('\\n')[1:])}\\n\\n\"\n",
    "        )\n",
    "        skill_count += 1\n",
    "\n",
    "    skill_count_prev = skill_count\n",
    "\n",
    "    # Step 2: Skill Discovery\n",
    "    discovery_rounds = 0\n",
    "    while len(selected_skills) > 0:\n",
    "        p_skill_discov = p_skill_discov_temp.render(SKILL_CONTEXT=skill_execution_context)\n",
    "        skill_discov_resp = agent_skill_exec_agent.chat(user_input=p_skill_discov, custom_system_prompt=p_default_system_temp.render())\n",
    "        selected_skills = skills_utils.parse_skills_from_json_response(json_response=skill_discov_resp, skills_hub_dir=skills_folder)\n",
    "\n",
    "        for skill_meta in selected_skills:\n",
    "            skill_execution_context += (\n",
    "                f\"SKill {skill_count + 1}: \\n\"\n",
    "                f\"{skill_meta['description']}\\n\"\n",
    "                f\"{'\\n'.join(skill_meta['body'].split('\\n')[1:])}\\n\\n\"\n",
    "            )\n",
    "            skill_count += 1\n",
    "        discovery_rounds += 1\n",
    "    new_skills_found = skill_count - skill_count_prev\n",
    "\n",
    "\n",
    "    # print(f\"End of skill discovery phase. Found total of new skills: {new_skills_found}\")\n",
    "    # Step 3: Query Execution\n",
    "    p_exec_insurBench_sys = p_skill_exec_temp.render(SKILL_CONTEXT=skill_execution_context)\n",
    "    insurBench_exec_response = agent_skill_exec_agent.chat(user_input=p_exec_insurBench, custom_system_prompt=p_exec_insurBench_sys)\n",
    "    message_classification = skills_utils.parse_message_from_json_response(insurBench_exec_response)\n",
    "    is_correct = true_label.lower() in message_classification.strip().lower()\n",
    "\n",
    "    predicted_label = get_insurBench_predicted_label(message_classification)\n",
    "    sample_end_time = time.time()\n",
    "    sample_elapsed = sample_end_time - sample_start_time\n",
    "    chat_history_agent_skill_select = agent_skill_aware.get_human_ai_message_history()\n",
    "    chat_history_agent_exec = agent_skill_exec_agent.get_human_ai_message_history()\n",
    "\n",
    "    # Build record and append to JSONL\n",
    "    record = {\n",
    "        \"index\": int(idx),\n",
    "        \"email_history\": email_history,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"raw_response\": message_classification,\n",
    "        \"correct\": is_correct,\n",
    "        \"selected_skills_step1\": selected_skill_names_step1,\n",
    "        \"hit_target_skill\": hit_target_skill,\n",
    "        \"new_skills_discovered\": new_skills_found,\n",
    "        \"discovery_rounds\": discovery_rounds,\n",
    "        \"elapsed_seconds\": round(sample_elapsed, 4),\n",
    "        \"model\": model_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"chat_history_agent_skill_select\": chat_history_agent_skill_select,\n",
    "        \"chat_history_agent_exec\": chat_history_agent_exec,\n",
    "        \"task_type\": \"agent_skill_based\"\n",
    "        \n",
    "    }\n",
    "\n",
    "    dataframe_record = pd.DataFrame([record])\n",
    "    dataframe_record.to_json(jsonl_path, orient=\"records\", lines=True, mode=\"a\" if os.path.exists(jsonl_path) else \"w\")\n",
    "    agent_skill_aware.clear_history()\n",
    "    agent_skill_exec_agent.clear_history()\n",
    "\n",
    "    ######################################################################################################################################\n",
    "    \n",
    "    sample_start_time, email_history, cls_label = time.time(), row.content, row.cls_label\n",
    "    true_label = {\"Non\": \"NO\", \"Oui\": \"YES\"}.get(cls_label, cls_label)\n",
    "    p_exec_insurBench_sys = p_default_system_temp.render()\n",
    "    insurBench_exec_response = agent_simple.chat(user_input=p_exec_insurBench, custom_system_prompt=p_exec_insurBench_sys)\n",
    "    message_classification = skills_utils.parse_message_from_json_response(insurBench_exec_response)\n",
    "    is_correct = true_label.lower() in message_classification.strip().lower()\n",
    "\n",
    "    predicted_label = get_prediction_XBRL_TAGS(message_classification)\n",
    "    sample_end_time = time.time()\n",
    "    sample_elapsed = sample_end_time - sample_start_time\n",
    "    chat_history_agent_exec = agent_simple.get_human_ai_message_history()\n",
    "\n",
    "    # Build record and append to JSONL\n",
    "    record = {\n",
    "        \"index\": int(idx),\n",
    "        \"email_history\": email_history,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"raw_response\": message_classification,\n",
    "        \"correct\": is_correct,\n",
    "        \"selected_skills_step1\": \"\",\n",
    "        \"hit_target_skill\": \"\",\n",
    "        \"new_skills_discovered\": \"\",\n",
    "        \"discovery_rounds\": \"\",\n",
    "        \"elapsed_seconds\": round(sample_elapsed, 4),\n",
    "        \"model\": model_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"chat_history_agent_skill_select\": \"\",\n",
    "        \"chat_history_agent_exec\": chat_history_agent_exec,\n",
    "        \"task_type\": \"agent_simple\"\n",
    "        \n",
    "    }\n",
    "\n",
    "    dataframe_record = pd.DataFrame([record])\n",
    "    dataframe_record.to_json(jsonl_path, orient=\"records\", lines=True, mode=\"a\" if os.path.exists(jsonl_path) else \"w\")\n",
    "    agent_simple.clear_history()\n",
    "\n",
    "\n",
    "    ######################################################################################################################################\n",
    "    sample_start_time, email_history, cls_label = time.time(), row.content, row.cls_Label\n",
    "    true_label = {\"Non\": \"NO\", \"Oui\": \"YES\"}.get(cls_label, cls_label)\n",
    "    skill_context_all =  \"The following are skills informaiton you can use as a reference for user request:\\n\".join([\n",
    "        f\"- **{skill['name']}**:\\n {skill['description']} **:\\n {skill['body']}\"\n",
    "        for skill in all_skills\n",
    "    ])\n",
    "    p_exec_insurBench = p_exec_insurBench_temp.render(EMAIL_HISTORY = email_history + skill_context_all)\n",
    "    p_exec_insurBench_sys = p_default_system_temp.render()\n",
    "\n",
    "    insurBench_exec_response = agent_simple.chat(user_input=p_exec_insurBench, custom_system_prompt=p_exec_insurBench_sys)\n",
    "    message_classification = skills_utils.parse_message_from_json_response(insurBench_exec_response)\n",
    "    is_correct = true_label.lower() in message_classification.strip().lower()\n",
    "\n",
    "    predicted_label = get_insurBench_predicted_label(message_classification)\n",
    "    sample_end_time = time.time()\n",
    "    sample_elapsed = sample_end_time - sample_start_time\n",
    "    chat_history_agent_exec = agent_simple.get_human_ai_message_history()\n",
    "\n",
    "    # Build record and append to JSONL\n",
    "    record = {\n",
    "        \"index\": int(idx),\n",
    "        \"email_history\": email_history,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"raw_response\": message_classification,\n",
    "        \"correct\": is_correct,\n",
    "        \"selected_skills_step1\": \"\",\n",
    "        \"hit_target_skill\": \"\",\n",
    "        \"new_skills_discovered\": \"\",\n",
    "        \"discovery_rounds\": \"\",\n",
    "        \"elapsed_seconds\": round(sample_elapsed, 4),\n",
    "        \"model\": model_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"chat_history_agent_skill_select\": \"\",\n",
    "        \"chat_history_agent_exec\": chat_history_agent_exec,\n",
    "        \"task_type\": \"agent_skill_full_context\"\n",
    "        \n",
    "    }\n",
    "\n",
    "    dataframe_record = pd.DataFrame([record])\n",
    "    dataframe_record.to_json(jsonl_path, orient=\"records\", lines=True, mode=\"a\" if os.path.exists(jsonl_path) else \"w\")\n",
    "    agent_simple.clear_history()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All {len(loaded_df)} samples processed. Results saved to: {jsonl_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3914661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_simple 0.5000 0.3333 0.0000 0.0888 8.8803\n",
      "agent_skill_based 0.5000 0.3333 1.0000 0.0707 7.0715\n",
      "agent_skill_full_context 0.5000 0.3333 0.0000 0.1525 15.2543\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score  # F1 supports macro/weighted averages [web:2]\n",
    "from LabAgentSkill.evaluate import get_insurBench_predicted_label_v2, get_insurBench_predicted_label, get_predicted_label, get_prediction_XBRL_TAGS\n",
    "\n",
    "jsonl_path = \"/home/snt/projects_lujun/LabAgentSkill/assets/results/insurBench_standard_gpt-4o-mini_20260212_163914.jsonl\"\n",
    "\n",
    "results_df = pd.read_json(jsonl_path, lines=True)\n",
    "TARGET = \"insurance-mail-triage\"\n",
    "def calc_hit_target_skill(row) -> bool:\n",
    "    lst = row.get(\"chat_history_agent_skill_select\", \"\")\n",
    "    if not lst:\n",
    "        return False\n",
    "    try:\n",
    "        obj = lst[1] \n",
    "        content = (obj or {}).get(\"content\", \"\")\n",
    "    except (IndexError, TypeError, AttributeError):\n",
    "        return False\n",
    "\n",
    "    return TARGET.lower() in str(content).lower()\n",
    "\n",
    "results_df[\"predicted_label\"] = results_df[\"predicted_label\"].apply(get_insurBench_predicted_label_v2)\n",
    "results_df[\"hit_target_skill\"] = results_df.apply(calc_hit_target_skill, axis=1)\n",
    "ESTIMATED_GPU_RAM_GB = 100\n",
    "\n",
    "for task_type, g in results_df.groupby(\"task_type\", dropna=False):\n",
    "\n",
    "    # --- ACC / F1: exclude \"unknown\" predictions ---\n",
    "    pred_lower = g[\"predicted_label\"].astype(str).str.lower()\n",
    "    valid_mask = pred_lower.ne(\"unknown\")\n",
    "    valid_df = g[valid_mask]\n",
    "    unknown_count = int((~valid_mask).sum())\n",
    "\n",
    "    if len(valid_df) > 0:\n",
    "        y_true = valid_df[\"true_label\"].astype(str).str.lower()\n",
    "        y_pred = valid_df[\"predicted_label\"].astype(str).str.lower()\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)      # macro avg [web:2]\n",
    "        f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)  # weighted avg [web:2]\n",
    "        correct = int((y_true == y_pred).sum())\n",
    "        denom = len(valid_df)\n",
    "    else:\n",
    "        acc = float(\"0.0000\")\n",
    "        f1_macro = float(\"0.0000\")\n",
    "        f1_weighted = float(\"0.0000\")\n",
    "        correct = 0\n",
    "        denom = 0\n",
    "\n",
    "    # --- Hit Rate: count non-empty hit_target_skill ---\n",
    "    hit_count = int(g[\"hit_target_skill\"].fillna(\"\").astype(str).eq(\"True\").sum())\n",
    "    hit_rate = hit_count / len(g) if len(g) > 0 else float(\"nan\")\n",
    "\n",
    "    # --- VRAM-Hours (GB·h): VRAM(GB) * time(hours) ---\n",
    "    total_time_sec = g[\"elapsed_seconds\"].fillna(0).astype(float).sum()\n",
    "    total_minutes = (total_time_sec / 60.0)\n",
    "    total_vram_minutes = total_minutes * ESTIMATED_GPU_RAM_GB\n",
    "    avg_minutes= total_minutes / len(g) if len(g) > 0 else float(\"nan\")\n",
    "    avg_vram_minutes = total_vram_minutes / len(g) if len(g) > 0 else float(\"nan\")\n",
    "    # --- Print only the requested metrics ---\n",
    "    # print(f\"[task_type={task_type}] \"\n",
    "    #       f\"ACC={acc:.4f} ({correct}/{denom}, unknown_excluded={unknown_count}) | \"\n",
    "    #       f\"F1_weighted={f1_weighted:.4f} | \"\n",
    "    #       f\"HitRate={hit_rate:.4f} ({hit_count}/{len(g)}) | \"\n",
    "    #       f\"AvgHours={avg_hours:.4f} | AvgVRAM-Hours={avg_vram_hours:.4f} GB·h\"\n",
    "    # )\n",
    "\n",
    "    print (f\"{task_type} {acc:.4f} {f1_weighted:.4f} {hit_rate:.4f} {avg_minutes:.4f} {avg_vram_minutes:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f3a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79fbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LabAgentSkill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
