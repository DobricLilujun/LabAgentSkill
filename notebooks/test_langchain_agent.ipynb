{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddb5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load ANTHROPIC_API_KEY from .env without printing it\n",
    "env_path = Path.cwd().parent / \".env\" if Path.cwd().name == \"notebooks\" else Path.cwd() / \".env\"\n",
    "if env_path.exists():\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        if key and key not in os.environ:\n",
    "            os.environ[key] = value\n",
    "\n",
    "assert \"ANTHROPIC_API_KEY\" in os.environ, \"ANTHROPIC_API_KEY is not set\"\n",
    "assert \"TAVILY_API_KEY\" in os.environ, \"TAVILY_API_KEY is not set\"\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af28a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='3cc12d20-ce6c-447b-b21a-0b3d4b129029'),\n",
       "  AIMessage(content=[{'id': 'toolu_018XEz7BYckBJdovLRSnqgZc', 'input': {'city': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01BnPVJBhUBpU66gmiLfg7bV', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'inference_geo': 'not_available', 'input_tokens': 569, 'output_tokens': 54, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019c2f37-e42b-7633-a5c7-730c06d59fae-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'toolu_018XEz7BYckBJdovLRSnqgZc', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 569, 'output_tokens': 54, 'total_tokens': 623, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='aa2d8b27-d676-4a08-9659-7f506bf57086', tool_call_id='toolu_018XEz7BYckBJdovLRSnqgZc'),\n",
       "  AIMessage(content='The weather in San Francisco is sunny! ☀️', additional_kwargs={}, response_metadata={'id': 'msg_01BTDALnBeXKoHf2uL9xXWUw', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'inference_geo': 'not_available', 'input_tokens': 643, 'output_tokens': 15, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019c2f37-ee83-7bc3-b8b1-6d5eaa4a66cc-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 643, 'output_tokens': 15, 'total_tokens': 658, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2842cdc",
   "metadata": {},
   "source": [
    "## Using Deep Agents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c262d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def internet_search(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return tavily_client.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic,\n",
    "    )\n",
    "\n",
    "# System prompt to steer the agent to be an expert researcher\n",
    "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
    "\n",
    "You have access to an internet search tool as your primary means of gathering information.\n",
    "\n",
    "## `internet_search`\n",
    "\n",
    "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
    "\"\"\"\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    tools=[internet_search],\n",
    "    system_prompt=research_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951d17a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on my research, here's what I found about **Lujun LI** at the University of Luxembourg:\n",
      "\n",
      "## Profile Summary\n",
      "\n",
      "**Lujun LI** is a **doctoral researcher** at the University of Luxembourg, affiliated with:\n",
      "- **Organization**: Interdisciplinary Centre for Security, Reliability and Trust (**SnT**)\n",
      "- **Department**: **SEDAN** (Software Engineering & Data Analytics)\n",
      "\n",
      "## Research Interests\n",
      "\n",
      "According to their Google Scholar and ResearchGate profiles, Lujun Li's research focuses on:\n",
      "- **Natural Language Processing**\n",
      "- **Large Language Models**\n",
      "- **Causal Modeling**\n",
      "- **Computer Vision**\n",
      "- **Statistics**\n",
      "- **Mathematics**\n",
      "\n",
      "## Affiliation\n",
      "\n",
      "Lujun Li appears to have a dual affiliation with both the **University of Luxembourg** and the **Foyer Group** (an insurance company in Luxembourg), suggesting they may be working on industry-academic collaborative research.\n",
      "\n",
      "## Recent Research Work\n",
      "\n",
      "Some of their recent publications include:\n",
      "1. **\"Exploring the Impact of Temperature on Large Language Models\"** - Research on LLM behavior\n",
      "2. **\"Vision Transformer-based Time-series Image Reconstruction for Cloud...\"** - Work on computer vision and remote sensing applications\n",
      "3. **\"Small Language Models in the Real World: Insights from Industrial...\"** - Practical applications of language models\n",
      "\n",
      "## Personal Note\n",
      "\n",
      "Their ResearchGate profile describes them as: *\"Part Time Runner, Full Time Dreamer\"*\n",
      "\n",
      "Lujun Li's email is: **lujun.li@uni.lu**\n",
      "\n",
      "Their work appears to bridge theoretical AI research with practical industrial applications, particularly in the areas of machine learning, natural language processing, and computer vision.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Who is Lujun LI in University Of Luxembourg?\"}]})\n",
    "\n",
    "# Print the agent's response\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from deepagents import create_deep_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "skill_url = \"https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md\"\n",
    "with urlopen(skill_url) as response:\n",
    "    skill_content = response.read().decode('utf-8')\n",
    "\n",
    "skills_files = {\n",
    "    \"/skills/langgraph-docs/SKILL.md\": skill_content\n",
    "}\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    skills=[\"./skills/\"],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is langgraph?\",\n",
    "            }\n",
    "        ],\n",
    "        # Seed the default StateBackend's in-state filesystem (virtual paths must start with \"/\").\n",
    "        \"files\": skills_files\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"12345\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a95f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "model = init_chat_model(model=\"gpt-4o-mini\")\n",
    "agent = create_deep_agent(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b00e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepagents import create_deep_agent\n",
    "\n",
    "research_instructions = \"\"\"\\\n",
    "You are an expert researcher. Your job is to conduct \\\n",
    "thorough research, and then write a polished report. \\\n",
    "\"\"\"\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    system_prompt=research_instructions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12d3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def internet_search(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return tavily_client.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic,\n",
    "    )\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    tools=[internet_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc417b5b",
   "metadata": {},
   "source": [
    "## Test SKILLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ab713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from deepagents import create_deep_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "skill_url = \"https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md\"\n",
    "with urlopen(skill_url) as response:\n",
    "    skill_content = response.read().decode('utf-8')\n",
    "\n",
    "skills_files = {\n",
    "    \"/skills/langgraph-docs/SKILL.md\": skill_content\n",
    "}\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    skills=[\"./skills/\"],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is langgraph?\",\n",
    "            }\n",
    "        ],\n",
    "        # Seed the default StateBackend's in-state filesystem (virtual paths must start with \"/\").\n",
    "        \"files\": skills_files\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"12345\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41fe93bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'files': {'/skills/langgraph-docs/SKILL.md': '---\\n'\n",
      "                                              'name: langgraph-docs\\n'\n",
      "                                              'description: Use this skill for requests related to LangGraph in order '\n",
      "                                              'to fetch relevant documentation to provide accurate, up-to-date '\n",
      "                                              'guidance.\\n'\n",
      "                                              '---\\n'\n",
      "                                              '\\n'\n",
      "                                              '# langgraph-docs\\n'\n",
      "                                              '\\n'\n",
      "                                              '## Overview\\n'\n",
      "                                              '\\n'\n",
      "                                              'This skill explains how to access LangGraph Python documentation to '\n",
      "                                              'help answer questions and guide implementation.\\n'\n",
      "                                              '\\n'\n",
      "                                              '## Instructions\\n'\n",
      "                                              '\\n'\n",
      "                                              '### 1. Fetch the Documentation Index\\n'\n",
      "                                              '\\n'\n",
      "                                              'Use the fetch_url tool to read the following URL:\\n'\n",
      "                                              'https://docs.langchain.com/llms.txt\\n'\n",
      "                                              '\\n'\n",
      "                                              'This provides a structured list of all available documentation with '\n",
      "                                              'descriptions.\\n'\n",
      "                                              '\\n'\n",
      "                                              '### 2. Select Relevant Documentation\\n'\n",
      "                                              '\\n'\n",
      "                                              'Based on the question, identify 2-4 most relevant documentation URLs '\n",
      "                                              'from the index. Prioritize:\\n'\n",
      "                                              '\\n'\n",
      "                                              '- Specific how-to guides for implementation questions\\n'\n",
      "                                              '- Core concept pages for understanding questions\\n'\n",
      "                                              '- Tutorials for end-to-end examples\\n'\n",
      "                                              '- Reference docs for API details\\n'\n",
      "                                              '\\n'\n",
      "                                              '### 3. Fetch Selected Documentation\\n'\n",
      "                                              '\\n'\n",
      "                                              'Use the fetch_url tool to read the selected documentation URLs.\\n'\n",
      "                                              '\\n'\n",
      "                                              '### 4. Provide Accurate Guidance\\n'\n",
      "                                              '\\n'\n",
      "                                              'After reading the documentation, complete the users request.\\n'},\n",
      " 'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='f2500ddd-5f81-4165-af5c-a91e9274dba8'),\n",
      "              AIMessage(content=\"LangGraph is a library for building stateful, multi-actor applications with Large Language Models (LLMs). It's part of the LangChain ecosystem and is designed to create agent and multi-agent workflows.\\n\\nHere are the key features of LangGraph:\\n\\n## Core Concepts\\n\\n1. **Graph-Based Architecture**: LangGraph uses a graph structure where nodes represent different computation steps and edges represent the flow of data between them.\\n\\n2. **State Management**: Unlike simple LLM chains, LangGraph provides sophisticated state management, allowing you to maintain context across multiple steps in a workflow.\\n\\n3. **Cycles and Loops**: It supports cyclical graphs, which is essential for agent workflows where you might need to loop back (e.g., an agent trying different approaches until it succeeds).\\n\\n4. **Human-in-the-Loop**: Built-in support for pausing execution to get human input or approval before proceeding.\\n\\n## Common Use Cases\\n\\n- **Agent Workflows**: Building agents that can plan, execute, and adapt their approach\\n- **Multi-Agent Systems**: Creating systems where multiple agents collaborate or compete\\n- **Complex Reasoning**: Implementing patterns like reflection, self-critique, or iterative refinement\\n- **Tool-Using Applications**: Orchestrating LLM calls with external tool usage\\n\\n## Key Differences from LangChain\\n\\nWhile LangChain focuses on linear chains and simple workflows, LangGraph is designed for:\\n- More complex, non-linear workflows\\n- Applications that need to maintain and update state\\n- Scenarios requiring conditional logic and loops\\n- Better control flow and error handling\\n\\nIt's particularly useful when you need more control and structure than a simple prompt-response pattern, making it ideal for production-grade AI applications.\", additional_kwargs={}, response_metadata={'id': 'msg_01QTuQ5s3PzYS5yqUKZFbAg4', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 6311}, 'cache_creation_input_tokens': 6311, 'cache_read_input_tokens': 0, 'inference_geo': 'not_available', 'input_tokens': 3, 'output_tokens': 383, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929', 'model_provider': 'anthropic'}, id='lc_run--019c2f5c-e272-7fa3-9a82-1b57ff346fa8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 6314, 'output_tokens': 383, 'total_tokens': 6697, 'input_token_details': {'cache_read': 0, 'cache_creation': 6311, 'ephemeral_5m_input_tokens': 6311, 'ephemeral_1h_input_tokens': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Pretty-print the result for easy reading\n",
    "if isinstance(result, dict):\n",
    "    try:\n",
    "        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    except TypeError:\n",
    "        pprint(result, width=120)\n",
    "else:\n",
    "    pprint(result, width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02efc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LabAgentSkill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
